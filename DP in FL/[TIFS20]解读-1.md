# Federated Learning with Differential Privacy: Algorithms and Performance Analysis

## NbAFL算法
这是一个DP+FL的算法，算法框架如下图所示。
![](/picture/2023-06-08-15-57-48.png)
算法分为如下几个阶段：
1. 客户端本地训练
2. 上传参数
3. 服务器进行参数聚合
4. 广播参数
5. 客户端本地测试

### DP实现
算法实现的是**全局**($\epsilon$,$\delta$)-DP，使用高斯机制，在2和4阶段添加高斯噪声实现。
在2阶段添加的高斯噪声尺度取决于本地训练集大小最小值m，本地参数暴露次数L，梯度裁剪范数C, $\epsilon$, $\delta$。
在4阶段的噪声比较特殊，通过分析，在训练轮数**T比较小**的时候，在2阶段添加的高斯噪声组合起来已经能对广播的聚合参数进行足够的扰动，不需要额外添加噪声；在**T比较大**的时候，可以添加一部分噪声达到差分隐私的目的。噪声大小取决于T, C, L, N, m, $\epsilon$, $\delta$。
P
### 收敛分析（Utility）
分析目标为E{F($\widetilde{w}^{(T)}$)-F($w^{*}$)}$\leq$?
最终通过分析，文章告诉我们，目标被一个上界bound住，且这个上界随着$\epsilon$的增大而减小，随着$N$的增大而减小，存在一个最优的T使得目标最小。
也就是说，模型泛化能力随着$\epsilon$的**增大**，$N$的**增大**而**变好**，而**对于T则不是单调**的,存在最优T。

![](/picture/2023-06-08-15-59-20.png)

分析过程当中，文章对本地损失函数和全局损失函数做了如下假设。

![](/picture/2023-06-08-16-04-29.png)

比较复杂，但是可以知道，常见的平方损失函数满足该假设。
